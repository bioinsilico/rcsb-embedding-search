import torch.nn as nn
from torch import norm
from collections import OrderedDict

from networks.layers import ResBlock


class LinearEmbeddingCosine(nn.Module):

    def __init__(self, input_features=1280, hidden_layer=1280):
        super().__init__()
        self.embedding = nn.Sequential(OrderedDict([
            ('norm', nn.LayerNorm(input_features)),
            ('dropout', nn.Dropout(p=0.0)),
            ('linear', nn.Linear(input_features, hidden_layer)),
            ('activation', nn.ReLU())
        ]))

    def forward(self, x, x_mask, y, y_mask):
        return nn.functional.cosine_similarity(
            self.embedding(x.sum(dim=1)),
            self.embedding(y.sum(dim=1))
        )

    def get_weights(self):
        return [(name, param) for name, param in self.embedding.named_parameters()]


class TransformerEmbeddingCosine(nn.Module):

    def __init__(
            self,
            input_features=1280,
            dim_feedforward=2048,
            hidden_layer=1280,
            nhead=10,
            num_layers=6,
            dropout=0.1
    ):
        super().__init__()
        self.dropout = dropout
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=input_features,
            nhead=nhead,
            dim_feedforward=dim_feedforward,
            dropout=self.dropout,
            batch_first=True
        )
        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)
        self.embedding = nn.Sequential(OrderedDict([
            ('norm', nn.LayerNorm(input_features)),
            ('dropout', nn.Dropout(p=self.dropout)),
            ('linear', nn.Linear(input_features, hidden_layer)),
            ('activation', nn.ReLU())
        ]))

    def forward(self, x, x_mask, y, y_mask):
        x_emb = self.embedding(self.transformer(x, src_key_padding_mask=x_mask).sum(dim=1))
        y_emb = self.embedding(self.transformer(y, src_key_padding_mask=y_mask).sum(dim=1))
        return nn.functional.cosine_similarity(
            self.embedding(x_emb),
            self.embedding(y_emb)
        ), norm(x_emb, p=2, dim=1), norm(y_emb, p=2, dim=1)

    def get_weights(self):
        return [(name, param) for name, param in self.embedding.named_parameters()]


class TransformerResBlockEmbeddingCosine(nn.Module):
    dropout = 0.1

    def __init__(self, input_features=1280, dim_feedforward=2048, hidden_layer=1280, nhead=8, num_layers=6):
        super().__init__()
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=input_features,
            nhead=nhead,
            dim_feedforward=dim_feedforward,
            dropout=self.dropout,
            batch_first=True
        )
        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)
        self.embedding = nn.Sequential(OrderedDict([
            ('block1', ResBlock(input_features, hidden_layer, self.dropout)),
            ('block2', ResBlock(input_features, hidden_layer, self.dropout)),
            ('block3', ResBlock(input_features, hidden_layer, self.dropout)),
            ('dropout', nn.Dropout(p=self.dropout)),
            ('linear', nn.Linear(input_features, hidden_layer)),
            ('activation', nn.ReLU())
        ]))

    def forward(self, x, x_mask, y, y_mask):
        return nn.functional.cosine_similarity(
            self.embedding(self.transformer(x, src_key_padding_mask=x_mask).sum(dim=1)),
            self.embedding(self.transformer(y, src_key_padding_mask=y_mask).sum(dim=1))
        )

    def get_weights(self):
        return [(name, param) for name, param in self.embedding.named_parameters()]


class TransformerEmbedding(nn.Module):
    dropout = 0.1

    def __init__(self, input_features=1280, dim_feedforward=2048, hidden_layer=1280, nhead=8, num_layers=6):
        super().__init__()
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=input_features,
            nhead=nhead,
            dim_feedforward=dim_feedforward,
            dropout=self.dropout,
            batch_first=True
        )
        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)
        self.embedding = nn.Sequential(OrderedDict([
            ('dropout', nn.Dropout(p=self.dropout)),
            ('linear', nn.Linear(input_features, hidden_layer)),
            ('activation', nn.ReLU())
        ]))

    def forward(self, x, x_mask):
        return self.embedding(self.transformer(x, src_key_padding_mask=x_mask).sum(dim=1))

    def get_weights(self):
        return [(name, param) for name, param in self.embedding.named_parameters()]

